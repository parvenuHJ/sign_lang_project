{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d2750b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rlarn\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers, layers, models, regularizers\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    GlobalAveragePooling1D,\n",
    "    LSTM, \n",
    "    Dense, \n",
    "    Input, \n",
    "    concatenate,\n",
    "    TimeDistributed,\n",
    "    Flatten\n",
    ")\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac6857ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/train'\n",
    "test_dir = 'data/test'\n",
    "# 비디오 파일 목록과 태그를 포함하는 리스트를 만드는 함수\n",
    "def create_data_list(data_dir):\n",
    "    data_list = []\n",
    "    # data_dir 안의 각 디렉토리에 대해 반복\n",
    "    for item in os.listdir(data_dir):\n",
    "        item_path = os.path.join(data_dir, item)  # 아이템의 전체 경로\n",
    "        # 해당 경로가 디렉토리인지 확인\n",
    "        if os.path.isdir(item_path):\n",
    "            # 디렉토리 내의 모든 파일을 나열\n",
    "            for file_name in os.listdir(item_path):\n",
    "                # 파일이 .mp4 파일인지 확인\n",
    "                if file_name.endswith('.mp4'):\n",
    "                    # 리스트에 태그와 파일 경로를 추가\n",
    "                    data_list.append((item, str(data_dir+'/'+item)+'/'+file_name))\n",
    "    return data_list\n",
    "\n",
    "# 함수를 사용해서 리스트를 생성\n",
    "train_list = create_data_list(train_dir)\n",
    "test_list = create_data_list(test_dir)\n",
    "# 리스트에서 데이터프레임을 생성\n",
    "train_df = pd.DataFrame(data=train_list, columns=['tag', 'video_name'])\n",
    "test_df = pd.DataFrame(data=test_list, columns=['tag', 'video_name'])\n",
    "# 필요한 경우 열 순서를 수정\n",
    "train_df = train_df.loc[:, ['tag', 'video_name']]\n",
    "test_df = test_df.loc[:, ['tag', 'video_name']]\n",
    "# 데이터프레임을 CSV 파일로 저장\n",
    "train_file_path = 'train.csv'\n",
    "test_file_path = 'test.csv'\n",
    "train_df.to_csv(train_file_path, encoding='utf-8-sig', index=False)\n",
    "test_df.to_csv(test_file_path, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1756281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total video for training: 50\n",
      "Total video for testing: 15\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "print(f\"Total video for training: {len(train_df)}\")\n",
    "print(f\"Total video for testing: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c4e3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "# GPU 장치 목록 가져오기\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    # GPU의 가상 메모리 제한을 6GB로 설정\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpu_devices[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 6)]\n",
    "    )\n",
    "    # set_memory_growth는 set_virtual_device_configuration과 함께 사용할 수 없습니다\n",
    "else:\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85a9b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "\n",
    "MAX_SEQ_LENGTH = 10\n",
    "NUM_FEATURES = 256\n",
    "SKELETON_FEATURES = 33*4\n",
    "HAND_FEATURES = 21*3*2\n",
    "ARM_ANGLE_FEATURES = 2\n",
    "N_CLASSES = len(np.unique(train_df[\"tag\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b79c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 이미지에서 중앙에 맞춰 정사각형으로 잘나내는 함수\n",
    "def crop_center_square(frame):\n",
    "    # 이미지의 높이(y)와 너비(x)를 가져옴\n",
    "    y, x = frame.shape[0:2]\n",
    "    # 이미지의 높이와 너비 중 더 작은 값을 선택하여 정사각형의 크기를 결정\n",
    "    min_dim = min(y, x)\n",
    "    # 정사각형을 이미지 중앙에 위치시키기 위해 시작점의 x좌표와 y좌표를 계산\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    # 계산된 시작점과 정사각형의 크기를 이용하여 이미지의 중앙 부분을 잘라냅니다.\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a2a39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a, b, c):\n",
    "    a = np.array([a.x, a.y])  # 첫 번째 점\n",
    "    b = np.array([b.x, b.y])  # 중간 점 (팔꿈치)\n",
    "    c = np.array([c.x, c.y])  # 세 번째 점 (손목)\n",
    "\n",
    "    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "\n",
    "    if angle > 180.0:\n",
    "        angle = 360 - angle\n",
    "\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2f81fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비디오 파일을 로드하고, 각 프레임을 처리하여 배열로 반환하는 함수\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, smooth_landmarks=True)\n",
    "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "    # OpenCV를 사용하여 비디오 파일 열기\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    skeletons = []  # 스켈레톤 데이터\n",
    "    hand_landmarks = []  # 손 데이터\n",
    "    arm_angles = []  # 팔 각도 데이터\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # 비디오에서 프레임을 하나씩 읽기\n",
    "            ret, frame = cap.read()\n",
    "            # 읽을 프레임이 없으면 반복문을 종료\n",
    "            if not ret:\n",
    "                break\n",
    "            # 읽은 프레임에서 중앙의 정사각형 부분을 잘라냄\n",
    "            frame = crop_center_square(frame)\n",
    "            # 프레임의 크기를 지정된 크기로 조절\n",
    "            frame = cv2.resize(frame, resize)            \n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Mediapipe를 사용하여 스켈레톤 추출\n",
    "            hands_results = hands.process(frame_rgb)\n",
    "            pose_results = pose.process(frame_rgb)\n",
    "           \n",
    "            if pose_results.pose_landmarks:\n",
    "                right_shoulder = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "                right_elbow = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_ELBOW]\n",
    "                right_wrist = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_WRIST]\n",
    "\n",
    "                left_shoulder = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "                left_elbow = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_ELBOW]\n",
    "                left_wrist = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "\n",
    "                right_arm_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
    "                left_arm_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "\n",
    "                arm_angles.append((right_arm_angle, left_arm_angle))\n",
    "                skeletons.append(pose_results.pose_landmarks.landmark)\n",
    "                mp.solutions.drawing_utils.draw_landmarks(\n",
    "                    frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)            \n",
    "                \n",
    "            if hands_results.multi_hand_landmarks:\n",
    "                hand_landmarks_data = hands_results.multi_hand_landmarks\n",
    "                hand_landmarks.append(hand_landmarks_data)\n",
    "                for hand_lm in hand_landmarks_data:\n",
    "                    mp.solutions.drawing_utils.draw_landmarks(\n",
    "                        frame, hand_lm, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            cv2.imshow('Video Frame', frame)\n",
    "            cv2.waitKey(30)\n",
    "            # OpenCV는 BGR 색상 순서를 사용하므로, 이를 RGB 순서로 변경\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            # 처리된 프레임을 프레임 리스트에 추가\n",
    "            frames.append(frame)\n",
    "            # max_frames가 지정된 경우, 지정된 수의 프레임만큼만 처리\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        # 비디오 파일을 닫기\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "        pose.close\n",
    "        hands.close\n",
    "    return np.array(frames), skeletons, hand_landmarks, np.array(arm_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed49f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 증강을 위한 함수 정의\n",
    "def augment_frame(frame, angle_range=5):\n",
    "    # 랜덤 회전을 위한 각도 설정\n",
    "    angle = random.uniform(-angle_range, angle_range)\n",
    "    # 회전 변환 행렬 계산\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((frame.shape[1] / 2, frame.shape[0] / 2), angle, 1)\n",
    "    # 프레임 회전\n",
    "    rotated_frame = cv2.warpAffine(frame, rotation_matrix, (frame.shape[1], frame.shape[0]))    \n",
    "    # 색상 조정 (여기서는 밝기 조정으로 예시를 듭니다)\n",
    "    adjusted_frame = cv2.convertScaleAbs(rotated_frame, alpha=1, beta=random.uniform(-30, 30))\n",
    "    \n",
    "    return noisy_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ad5d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 증강용\n",
    "def load_augment_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, smooth_landmarks=True)\n",
    "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "    # OpenCV를 사용하여 비디오 파일 열기\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    skeletons = []  # 스켈레톤 데이터\n",
    "    hand_landmarks = []  # 손 데이터\n",
    "    arm_angles = []  # 팔 각도 데이터\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # 비디오에서 프레임을 하나씩 읽기\n",
    "            ret, frame = cap.read()\n",
    "            # 읽을 프레임이 없으면 반복문을 종료\n",
    "            if not ret:\n",
    "                break\n",
    "            # 읽은 프레임에서 중앙의 정사각형 부분을 잘라냄\n",
    "            frame = crop_center_square(frame)\n",
    "            # 프레임의 크기를 지정된 크기로 조절\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = augment_frame(frame)\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Mediapipe를 사용하여 스켈레톤 추출\n",
    "            hands_results = hands.process(frame_rgb)\n",
    "            pose_results = pose.process(frame_rgb)\n",
    "           \n",
    "            if pose_results.pose_landmarks:\n",
    "                right_shoulder = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "                right_elbow = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_ELBOW]\n",
    "                right_wrist = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_WRIST]\n",
    "\n",
    "                left_shoulder = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "                left_elbow = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_ELBOW]\n",
    "                left_wrist = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "\n",
    "                right_arm_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
    "                left_arm_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "\n",
    "                arm_angles.append((right_arm_angle, left_arm_angle))\n",
    "                skeletons.append(pose_results.pose_landmarks.landmark)\n",
    "                mp.solutions.drawing_utils.draw_landmarks(\n",
    "                    frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)            \n",
    "                \n",
    "            if hands_results.multi_hand_landmarks:\n",
    "                hand_landmarks_data = hands_results.multi_hand_landmarks\n",
    "                hand_landmarks.append(hand_landmarks_data)\n",
    "                for hand_lm in hand_landmarks_data:\n",
    "                    mp.solutions.drawing_utils.draw_landmarks(\n",
    "                        frame, hand_lm, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            cv2.imshow('Video Frame', frame)\n",
    "            cv2.waitKey(30)\n",
    "            # OpenCV는 BGR 색상 순서를 사용하므로, 이를 RGB 순서로 변경\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            # 처리된 프레임을 프레임 리스트에 추가\n",
    "            frames.append(frame)\n",
    "            # max_frames가 지정된 경우, 지정된 수의 프레임만큼만 처리\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        # 비디오 파일을 닫기\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "        pose.close\n",
    "        hands.close\n",
    "    return np.array(frames), skeletons, hand_landmarks, np.array(arm_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "226bfeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특징추출\n",
    "def build_feature_extractor():\n",
    "    # 이미지 특징 추출을 위한 EfficientNetB0 모델\n",
    "    base_model = EfficientNetB0(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.efficientnet.preprocess_input\n",
    "    image_input = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed_image = preprocess_input(image_input)\n",
    "    image_features = base_model(preprocessed_image)\n",
    "\n",
    "    # Mediapipe 데이터 처리\n",
    "    mediapipe_input = keras.Input((258,))\n",
    "    mediapipe_features = keras.layers.Dense(258, activation=\"relu\")(mediapipe_input)\n",
    "    mediapipe_features = keras.layers.Dropout(0.3)(mediapipe_features)  # Dropout 추가\n",
    "\n",
    "    # 팔 각도 데이터 처리\n",
    "    arm_angle_input = keras.Input((2,))\n",
    "    arm_angle_features = keras.layers.Dense(16, activation=\"relu\")(arm_angle_input)\n",
    "    arm_angle_features = keras.layers.Dropout(0.3)(arm_angle_features)  # Dropout 추가\n",
    "\n",
    "    # 데이터 결합 및 추가 처리\n",
    "    combined_features = keras.layers.concatenate(\n",
    "        [image_features, mediapipe_features, arm_angle_features])\n",
    "    combined_features = keras.layers.BatchNormalization()(combined_features)\n",
    "    combined_features = keras.layers.Dense(\n",
    "        256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(combined_features)\n",
    "\n",
    "    return keras.Model(inputs=[image_input, mediapipe_input, arm_angle_input], outputs=combined_features, name=\"feature_extractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a307a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손 랜드마크를 2개로 제한한 코드\n",
    "def preprocess_skeleton_data(skeleton):\n",
    "    # 스켈레톤 데이터가 없는 경우 빈 벡터 반환\n",
    "    if not skeleton:\n",
    "        return np.zeros(SKELETON_FEATURES)\n",
    "    # 스켈레톤 데이터를 1차원 배열로 변환\n",
    "    skeleton_array = np.array([[lm.x, lm.y, lm.z] for lm in skeleton]).flatten()    \n",
    "    # 부족한 부분을 0으로 채우기\n",
    "    skeleton_array = np.pad(skeleton_array, ((0, max(0, SKELETON_FEATURES - len(skeleton_array)))))    \n",
    "    return skeleton_array\n",
    "\n",
    "def preprocess_hand_data(hand_landmarks):\n",
    "    # 손 랜드마크 데이터가 없는 경우 빈 벡터 반환\n",
    "    if not hand_landmarks or len(hand_landmarks) < 2:\n",
    "        return np.zeros(HAND_FEATURES)\n",
    "    \n",
    "    # 첫 번째와 두 번째 손에 대한 랜드마크만 처리\n",
    "    hand_data = []\n",
    "    for hand_lm in hand_landmarks[:2]:  # 첫 번째와 두 번째 손만 처리\n",
    "        lm_array = np.array([[lm.x, lm.y, lm.z] for lm in hand_lm.landmark]).flatten()\n",
    "        hand_data.extend(lm_array)\n",
    "\n",
    "    # 부족한 부분을 0으로 채우기\n",
    "    hand_data = np.pad(hand_data, ((0, max(0, HAND_FEATURES - len(hand_data)))))\n",
    "\n",
    "    return np.array(hand_data)\n",
    "\n",
    "def preprocess_arm_angle_data(arm_angles):\n",
    "    # 팔 각도 데이터가 없는 경우 빈 벡터 반환\n",
    "    if not arm_angles.size == 0:\n",
    "        return np.zeros(2)  # 오른팔, 왼팔 각각의 각도\n",
    "    return np.array(arm_angles)\n",
    "\n",
    "def preprocess_image(frame):\n",
    "    # frame을 이미지 배열로 변환\n",
    "    frame = image.img_to_array(frame)\n",
    "    # EfficientNetB0에 맞는 전처리 적용\n",
    "    frame = preprocess_input(frame)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ae3a9d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_all_video(df):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "\n",
    "    # Mediapipe 데이터를 저장할 배열 초기화                          \n",
    "    frame_skeletons = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, SKELETON_FEATURES), dtype=\"float16\")\n",
    "    frame_hands = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, HAND_FEATURES), dtype=\"float16\")    \n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float16\")\n",
    "    frame_images = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, IMG_SIZE, IMG_SIZE, 3), dtype=\"float16\")\n",
    "    frame_arm_angles = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, ARM_ANGLE_FEATURES), dtype=\"float16\")  # 2는 팔 각도 데이터 차원\n",
    "    \n",
    "    # 특징 추출기 모델 초기화\n",
    "    feature_extractor = build_feature_extractor()\n",
    "    \n",
    "    load_list = [\"load_video\",\"load_augment_video\"]\n",
    "    for load_func_name in load_list:\n",
    "        load_func = globals().get(load_func_name)\n",
    "        for idx, path in enumerate(video_paths):\n",
    "            frames, skeletons, hands, arm_angles = load_func(path)\n",
    "            video_length = min(MAX_SEQ_LENGTH, len(frames))\n",
    "\n",
    "            for i in range(video_length):\n",
    "                # 이미지 데이터 전처리 및 특징 추출\n",
    "                image_feature = preprocess_image(frames[i])\n",
    "                image_feature = np.expand_dims(image_feature, axis=0)\n",
    "\n",
    "                # Mediapipe 데이터 전처리\n",
    "                skeleton_feature = preprocess_skeleton_data(skeletons[i])\n",
    "                hand_feature = preprocess_hand_data(hands[i])\n",
    "                combined_mediapipe_data = np.concatenate([skeleton_feature, hand_feature])\n",
    "                combined_mediapipe_data = np.expand_dims(combined_mediapipe_data, axis=0)\n",
    "\n",
    "                # 팔 각도 데이터 전처리\n",
    "                arm_angle_feature = preprocess_arm_angle_data(arm_angles[i])\n",
    "                arm_angle_feature = np.expand_dims(arm_angle_feature, axis=0)\n",
    "\n",
    "                # 모델 예측\n",
    "                try:\n",
    "                    # 특징 추출기 모델에 이미지, Mediapipe 데이터, 팔 각도 데이터 전달\n",
    "                    frame_feature = feature_extractor.predict([image_feature, combined_mediapipe_data, arm_angle_feature], verbose=0)\n",
    "                    frame_features[idx, i, :] = frame_feature.squeeze()  # 예측 결과 저장, .squeeze()로 불필요한 차원 제거\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during prediction at index {idx}, frame {i}: {e}\")\n",
    "                    # 오류 발생시 해당 프레임을 0으로 설정하거나 다른 처리를 수행\n",
    "                    frame_features[idx, i, :] = np.zeros(NUM_FEATURES)\n",
    "                    frame_masks[idx, i] = 0  # 오류가 발생한 프레임은 마스크에서 제외\n",
    "\n",
    "                # 데이터 저장\n",
    "                frame_images[idx, i, :] = frames[i]  # 원본 이미지 데이터 저장\n",
    "                frame_skeletons[idx, i, :] = skeleton_feature\n",
    "                frame_hands[idx, i, :] = hand_feature\n",
    "                frame_arm_angles[idx, i, :] = arm_angle_feature\n",
    "                frame_masks[idx, i] = 1\n",
    "            \n",
    "    # 반환 값에 Mediapipe 데이터 포함\n",
    "    return (frame_features, frame_skeletons, frame_hands, frame_arm_angles, frame_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8221829b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rlarn\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\rlarn\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m prepare_all_video(train_df)\n",
      "Cell \u001b[1;32mIn[13], line 20\u001b[0m, in \u001b[0;36mprepare_all_video\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     18\u001b[0m load_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()\u001b[38;5;241m.\u001b[39mget(load_func_name)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(video_paths):\n\u001b[1;32m---> 20\u001b[0m     frames, skeletons, hands, arm_angles \u001b[38;5;241m=\u001b[39m load_func(path)\n\u001b[0;32m     21\u001b[0m     video_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(MAX_SEQ_LENGTH, \u001b[38;5;28mlen\u001b[39m(frames))\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(video_length):\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;66;03m# 이미지 데이터 전처리 및 특징 추출\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m, in \u001b[0;36mload_video\u001b[1;34m(path, max_frames, resize)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Mediapipe를 사용하여 스켈레톤 추출\u001b[39;00m\n\u001b[0;32m     29\u001b[0m hands_results \u001b[38;5;241m=\u001b[39m hands\u001b[38;5;241m.\u001b[39mprocess(frame_rgb)\n\u001b[1;32m---> 30\u001b[0m pose_results \u001b[38;5;241m=\u001b[39m pose\u001b[38;5;241m.\u001b[39mprocess(frame_rgb)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pose_results\u001b[38;5;241m.\u001b[39mpose_landmarks:\n\u001b[0;32m     33\u001b[0m     right_shoulder \u001b[38;5;241m=\u001b[39m pose_results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark[mp_pose\u001b[38;5;241m.\u001b[39mPoseLandmark\u001b[38;5;241m.\u001b[39mRIGHT_SHOULDER]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mprocess(input_data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image})\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\mediapipe\\python\\solution_base.py:372\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    366\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    368\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    369\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    370\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mwait_until_idle()\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = prepare_all_video(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8144026",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\n",
    "train_labels = train_df[\"tag\"].values\n",
    "train_labels = train_label_processor(train_labels[..., None]).numpy()\n",
    "train_labels = np.squeeze(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65297a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = prepare_all_video(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(test_df[\"tag\"]))\n",
    "test_labels = test_df[\"tag\"].values\n",
    "test_labels = test_label_processor(test_labels[..., None]).numpy()\n",
    "test_labels = np.squeeze(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0737a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 파일에 저장\n",
    "with open('train_data.pkl', 'wb') as file:\n",
    "    pickle.dump(train_data, file)\n",
    "with open('test_data.pkl', 'wb') as file:\n",
    "    pickle.dump(test_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_data.pkl', 'rb') as file:\n",
    "    train_data = pickle.load(file)\n",
    "with open('test_data.pkl', 'rb') as file:\n",
    "    test_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686df2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_model():\n",
    "    class_vocab = train_label_processor.get_vocabulary()\n",
    "    \n",
    "    # 입력 레이어\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    skeleton_input = keras.Input((MAX_SEQ_LENGTH, SKELETON_FEATURES))\n",
    "    hand_input = keras.Input((MAX_SEQ_LENGTH, HAND_FEATURES))\n",
    "    arm_angle_input = keras.Input((MAX_SEQ_LENGTH, ARM_ANGLE_FEATURES))  # 팔 각도 데이터 입력 레이어\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    \n",
    "    # LSTM 레이어\n",
    "    x = LSTM(32, return_sequences=True)(frame_features_input, mask=mask_input)\n",
    "    x = LSTM(16, return_sequences=False)(x)  # 시퀀스의 마지막 출력만 사용    \n",
    "\n",
    "    y_skeleton = GlobalAveragePooling1D()(skeleton_input)\n",
    "    y_hand = GlobalAveragePooling1D()(hand_input)\n",
    "    y_arm_angle = GlobalAveragePooling1D()(arm_angle_input)\n",
    "    \n",
    "    combined = concatenate([x, y_skeleton, y_hand, y_arm_angle])\n",
    "\n",
    "    # 추가 처리 및 출력 레이어\n",
    "    z = Dense(16, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(combined)\n",
    "    output = Dense(len(class_vocab), activation=\"softmax\", kernel_regularizer=regularizers.l2(0.01))(z)\n",
    "    \n",
    "    # 모델 생성 및 컴파일\n",
    "    lstm_model = keras.Model([frame_features_input, skeleton_input, hand_input, arm_angle_input, mask_input], output)\n",
    "    lstm_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c13067e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE= 64\n",
    "def run_experiment():\n",
    "    filepath = \"tmp/video_classifier_lstm.h5\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1)\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    # 모델 학습\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1], train_data[2], train_data[3], train_data[4]],\n",
    "        train_labels,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    \n",
    "    # 테스트 데이터에 대한 평가\n",
    "    _, accuracy = seq_model.evaluate(\n",
    "        [test_data[0], test_data[1], test_data[2], test_data[3], test_data[4]],\n",
    "        test_labels\n",
    "    )\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    \n",
    "    # 손실 및 정확도 그래프 출력\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    # 손실 그래프\n",
    "    ax1 = plt.subplot(1, 1, 1)\n",
    "    ax1.plot(history.history['loss'], label='Train Loss', color='blue')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss', color='blue', linestyle='dashed')\n",
    "    ax1.set_title('Training and Validation Loss and Accuracy')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax1.legend(loc='upper left')\n",
    "    # 정확도 그래프를 같은 그래프에 추가\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(history.history['accuracy'], label='Train Accuracy', color='green')\n",
    "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', color='green', linestyle='dashed')\n",
    "    ax2.set_ylabel('Accuracy', color='green')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "    ax2.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    seq_model.save('lstm_model.h5')\n",
    "    \n",
    "    return history, seq_model\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_video(frames, skeletons, hands, arm_angles):\n",
    "    num_frames = len(frames)\n",
    "    video_length = min(MAX_SEQ_LENGTH, num_frames)\n",
    "\n",
    "    # 데이터 초기화\n",
    "    frame_mask = np.zeros((1, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros((1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float16\")\n",
    "    frame_skeletons = np.zeros((1, MAX_SEQ_LENGTH, SKELETON_FEATURES), dtype=\"float16\")\n",
    "    frame_hands = np.zeros((1, MAX_SEQ_LENGTH, HAND_FEATURES), dtype=\"float16\")\n",
    "    frame_arm_angles = np.zeros((1, MAX_SEQ_LENGTH, ARM_ANGLE_FEATURES), dtype=\"float16\")\n",
    "    \n",
    "    # EfficientNetB0 모델을 사용하여 이미지 특징 추출\n",
    "    feature_extractor = Sequential([\n",
    "        EfficientNetB0(include_top=False, weights='imagenet', pooling='avg'),\n",
    "        Dense(NUM_FEATURES, activation='relu')\n",
    "    ])\n",
    "    \n",
    "    for j in range(video_length):\n",
    "        # 이미지 데이터 전처리 및 특징 추출\n",
    "        image_feature = preprocess_image(frames[j])\n",
    "        image_feature = np.expand_dims(image_feature, axis=0)\n",
    "        feature_result = feature_extractor.predict(image_feature)\n",
    "\n",
    "        # 특징 저장\n",
    "        frame_features[0, j] = feature_result\n",
    "\n",
    "        # Mediapipe 데이터 전처리\n",
    "        skeleton_feature = preprocess_skeleton_data(skeletons[j])\n",
    "        hand_feature = preprocess_hand_data(hands[j])        \n",
    "        arm_angle_feature = np.array(arm_angles[j])\n",
    "\n",
    "        # 데이터 저장\n",
    "        frame_skeletons[0, j] = skeleton_feature\n",
    "        frame_hands[0, j] = hand_feature\n",
    "        frame_mask[0, j] = 1        \n",
    "        frame_arm_angles[0, j] = arm_angle_feature\n",
    "\n",
    "    return frame_features, frame_skeletons, frame_hands, frame_mask, frame_arm_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a5d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_prediction(path, sequence_model):\n",
    "    class_vocab = test_label_processor.get_vocabulary()\n",
    "    frames, skeletons, hands, arm_angles = load_video(path)    \n",
    "    try:\n",
    "        num_frames = len(frames)\n",
    "    except IndexError:\n",
    "        print(\"Error: Unable to determine the number of frames. Frames shape:\", frames.shape)\n",
    "        return None\n",
    "    \n",
    "    # EfficientNetB0 모델을 사용하여 프레임에서 특징 추출\n",
    "    frame_features, frame_skeletons, frame_hands, frame_mask, frame_arm_angles = prepare_single_video(frames, skeletons, hands, arm_angles)\n",
    "    \n",
    "    # LSTM 모델을 사용하여 예측 수행\n",
    "    probabilities = sequence_model.predict([frame_features, frame_skeletons, frame_hands, frame_arm_angles, frame_mask])[0]\n",
    "    \n",
    "    # 가장 가능성 높은 레이블 출력\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"{class_vocab[i]} : {probabilities[i] * 100:5.2f}%\")\n",
    "    \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73081774",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "\n",
    "test_frames = sequence_prediction(test_video,sequence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdfe2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
